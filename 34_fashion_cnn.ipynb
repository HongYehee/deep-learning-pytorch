{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasion MNIST (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, MSELoss, Sequential, Sigmoid, Module\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(x_train, y_train),(x_test,y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of label name\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 24, 24])\n",
      "flatten torch.Size([1, 17280])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0709, -0.0916]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten\n",
    "sample_img = torch.zeros(1, 1, 28, 28)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=30, kernel_size=5, stride=1)\n",
    "linear = nn.Linear(17280, 2) # z = matmul(x, w) + b / w: 17280 * 2 / b: 2\n",
    "x = conv1(sample_img)\n",
    "print(x.shape) # (28 - 5) / 1 + 1\n",
    "x = torch.flatten(x, 1) # x.view(1, 30 * 24 * 24)\n",
    "print('flatten', x.shape)\n",
    "linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=30, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=60, kernel_size=5, stride=1)        \n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2)\n",
    "        example_input = torch.zeros(1, 1, 28, 28)  # MNIST image size\n",
    "        flattened_size = self._get_flattened_size(example_input)\n",
    "        print('flattened_size=', flattened_size)\n",
    "        self.fc1 = nn.Linear(flattened_size, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.smax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        with torch.no_grad(): # 학습과 관계없는 텐서 크기 계산이므로 no_grad 사용\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x =  self.max1(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            return torch.flatten(x, 1).shape[1]  \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x =  self.max1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.smax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.IntTensor( [[1,2],[3,4]])\n",
    "t = torch.IntTensor( [0,1])\n",
    "pred = test.max(dim=1)[1] # 행에서 가장 큰 값의 인덱스\n",
    "print(pred)\n",
    "(pred == t).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wsadm\\AppData\\Local\\Temp\\ipykernel_15088\\1167537053.py:3: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  y = torch.LongTensor(y_train)\n"
     ]
    }
   ],
   "source": [
    "x1 = x_train.reshape(-1,1,28,28)\n",
    "x = torch.FloatTensor(x1/255)\n",
    "y = torch.LongTensor(y_train)\n",
    "\n",
    "dataset = TensorDataset(x, y)  # TensorDataset으로 데이터셋 생성\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)  # DataLoader로 배치 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattened_size= 6000\n",
      "정확도 60.84\n",
      "정확도 65.14833333333333\n",
      "정확도 56.19166666666667\n",
      "정확도 63.63166666666667\n",
      "정확도 59.945\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = CNN().to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam( model.parameters(), lr=0.01)\n",
    "\n",
    "for step in range(5):\n",
    "    model.train()\n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        hx = model(batch_x)  # 배치 데이터로 예측\n",
    "        cost = loss_fn(hx, batch_y)  # 손실 계산\n",
    "        cost.backward()  # 역전파\n",
    "        optimizer.step()  # 가중치 업데이트\n",
    "        # print('batch',batch_idx)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad() :\n",
    "        # 데이터의 수 만큼 반복한다.\n",
    "        for i, (data, target) in enumerate(dataloader) :\n",
    "            # data = data\n",
    "            # target = target\n",
    "            output = model(data)\n",
    "\n",
    "            # pred = output.max(1, keepdim=True)[1]\n",
    "            # a = pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred = output.max(dim=1)[1]\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "    # 맞춘 개수를 전체 개수로 나눠 맞춘 확률을 구한다.\n",
    "    test_accuracy = 100.0 * correct / len(dataloader.dataset) \n",
    "    print('정확도', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
